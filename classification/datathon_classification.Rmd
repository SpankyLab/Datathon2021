---
title: "Datathon Classification"
author: "Ryuta Yoshimatsu"
output:
  html_document: 
    number_sections: true
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load Libraries

To **install** and add new package, run `install.packages('package_name')` in **RStudio Console** and add a line `library('package_name')` below

```{r packages, message = FALSE}
library(statsr)
library(dplyr)
library(MASS)
library(BAS)
library(ggplot2)
library(devtools)
library(gridExtra)
library(grid)
library(GGally)
library(PreProcess)
library(tidyverse)
library(knitr)
```

# Load Data

There are two data sets. `train` contains the dependent variable `Survived`.

```{r load}
train <- read.csv("classification_train.csv")
test <- read.csv("classification_test.csv")
```

# First Look at the Data

## Structures and Dimensions 

Let's first take a look at the **structure** (columns and data types) and the **dimension** (number of rows and columns) of the data sets.

```{r}
str(train)
```

```{r}
str(test)
```

We will add column `Survived` in `test` data set and populate it with `NA`. It is the column that we eventually want to predict. 

```{r}
test$Survived <- NA
```

The dimensions of `train` and `test` data sets are now the same. We will bind them together and create `all` data set.

```{r}
all <- rbind(train, test)
```

We will run descriptive analytics (distribution, correlation, etc.) on `all` data set instead of doing this separately to `train` and `test`. Later on, before we start modeling, we split `all` back into `train` and `test`.

We also want to know which columns are numerical and which are categorical.

```{r}
all %>% select_if(is.numeric) %>% colnames()
all %>% select_if(negate(is.numeric)) %>% colnames()
```

**Summary**: there are **1,309 rows** with **12 columns** in `all` data set including **891** rows from `train` and **418** rows from `test`, of which **7 are numerical** and **5 are categorical** variables.

## Completeness

We check for the **completeness** of the data: i.e. to check if there are any columns with `NA` values.

```{r}
nacolumn <- which(colSums(is.na(all)) > 0)
sort(colSums(sapply(all[nacolumn], is.na)), decreasing=TRUE)
```

418 entries missing `Survived` are from `test`. Besides, there are 263 entries missing `Age` and 1 entry missing `Fare`.

### Impute Missing Values: `Age` and `Fare`

**`Age`**

The rule of sum for using a statistical imputation on a column with missing values is that the proportion of missing values in under 5%. In this case, we have about to 15% entries with missing `Age`. It's probably better to drop the entries with NA values in `Age`, but we will use an imputation method called `stochastic regression imputation` for demonstration purpose.

```{r, fig.width=12, fig.height=4}
library(mice)

# Stochastic regression imputation on column Age
set.seed(1)
imp_df <- data.frame(is.na(all))
imp_df$Fare <- FALSE
imp_df <- imp_df %>% dplyr::select(-Survived)
imp <- mice(all[-2], method="norm.nob", m=1, where=imp_df)
all$Age <- complete(imp)$Age

# Setting the minimum boundary for Age at 0
all$Age[all$Age < 0] <- 0
```

**`Fare`**

We impute the missing `Fare`. If this entry was in `train`, we would have dropped it. But since it's in `test` (`Survived` = NA) and some models can't handle NA values, we perform imputation.

```{r}
all %>% filter(is.na(Fare))
```

The passenger with an empty `Fare` value is a single male (`Sex` = male, `SibSp` = 0, `Parch` = 0), who embarked at Southampton on the 3rd class. We give the **median** `Fare` value of the `Fare` paid by the passenger from the same group.

```{r}
male_single_southamplton <- all %>% filter(Sex=='male' & SibSp==0 & Parch==0 & Pclass==3 & Embarked=='S') %>% dplyr::select(Fare)
median <- median(male_single_southamplton$Fare, na.rm=TRUE)
all$Fare[is.na(all$Fare)] <- median
all %>% filter(PassengerId==1044)
```

# Exploratory Data Analysis

To understand the data and build some intuition, we run an exploratory data analysis. There are many ways to do this but below we will look at:

  1. the distribution of each variables
  2. the correlation between each variables

## Distributions

The distribution of a numerical variable is typically visualized using a **histogram**, whereas for a categorical variable, **bar plots** (frequency count) are often used. 

### Dependent Variable: `Survived`

We first take a look into our dependent variable `Survived`. But before that we will convert `Survived` to a factor column

```{r, fig.width=4, fig.height=4}
all$Survived <- as.factor(all$Survived)
ggplot(na.omit(all), aes(Survived)) + geom_bar(alpha=0.5, fill='blue') + geom_label(stat = "count", aes(label = ..count.., y = ..count..), size=3)
```

Out of 891 passengers, 549 (62%) died and 342 (38%) survived.

### Independent Numerical Variables: `Age` and `Fare`

```{r, fig.width=10, fig.height=3.5}
h1 <- ggplot(na.omit(all), aes(Age)) + geom_histogram(data=na.omit(all), bins=30, fill='blue', alpha=0.5)
h2 <- ggplot(na.omit(all), aes(Fare)) + geom_histogram(data=na.omit(all), bins=50, fill='blue', alpha=0.5)
grid.arrange(
  h1, h2,
  nrow=1,
  bottom = textGrob(
    "",
    gp = gpar(fontface=3, fontsize=9),
    hjust=1,
    x=1
  )
)
```

Both distributions are right skewed.

### Independent Categorical Variables with Low Cardinality: `Pclass`, `SibSp`, `Parch`, `Sex` and `Embarked`

```{r,  fig.width=8, fig.height=8}
b1 <- ggplot(all, aes(Pclass)) + geom_bar(alpha=0.5,fill='blue') + geom_label(stat = "count", aes(label = ..count.., y = ..count..), size=3)
b2 <- ggplot(all, aes(SibSp)) + geom_bar(alpha=0.5,fill='blue') + geom_label(stat = "count", aes(label = ..count.., y = ..count..), size=3)
b3 <- ggplot(all, aes(Parch)) + geom_bar(alpha=0.5,fill='blue') + geom_label(stat = "count", aes(label = ..count.., y = ..count..), size=3)
b4 <- ggplot(all, aes(Sex)) + geom_bar(alpha=0.5,fill='blue') + geom_label(stat = "count", aes(label = ..count.., y = ..count..), size=3)
b5 <- ggplot(all, aes(Embarked)) + geom_bar(alpha=0.5,fill='blue') + geom_label(stat = "count", aes(label = ..count.., y = ..count..), size=3)
grid.arrange(
  b1, b2, b3, b4, b5,
  nrow=3,
  bottom = textGrob(
    "",
    gp = gpar(fontface=3, fontsize=9),
    hjust=1,
    x=1
  )
)
```

### Independent Categorical Variables with High Cardinality: `Name`, `Ticket` and `Cabin`

We will check for the uniqueness for the independent categorical variables with high cardinality.

**`Name`**

```{r}
length(all$Name)
length(unique(all$Name))
all %>% group_by(Name) %>% dplyr::summarise(n=n()) %>% filter(n>1)
all %>% filter(Name=='Connolly, Miss. Kate' | Name=='Kelly, Mr. James') %>% arrange(Name)
```

`Name` is unique except for `Connolly, Miss. Kate` and `Kelly, Mr. James`. Each of these names has 2 entries in the data set. Since they are relatively common names and other column values look different enough, we consider them as different passengers. 

**`Ticket`**

```{r}
length(unique(all$Ticket))
all %>% group_by(Ticket) %>% dplyr::summarise(n=n()) %>% filter(n>1) %>% arrange(desc(n)) %>% head()
```

`Ticket` values are not unique. Some families/passengers shared the same ticket.

**`Cabin`**

```{r}
all %>% group_by(Cabin) %>% dplyr::summarise(n=n()) %>% arrange(desc(n)) %>% head()
all %>% filter(Cabin != '') %>% head()
```

There are 1014 entries with missing `Cabin` (i.e. empty string stored in the column).

## Correlations

### Dependent Variable vs. Independent Variables

Analyzing the correlations between the dependent variable and the independent variables tells us which independent variables could be a strong predictor. 

#### Dependent Variable (`Survived`) vs. Numerical Independent Variables (`Age`, `Fare`)

```{r, fig.width=7.5, fig.height=6.5, warning=FALSE}
b1 <- ggplot(aes(x=Survived, y=Age), data=na.omit(all)) + geom_boxplot(alpha=0.25, fill='blue', color='blue') +  xlab("Survived")
b2 <- ggplot(aes(x=Survived, y=Fare), data=na.omit(all)) + geom_boxplot(alpha=0.25, fill='blue', color='blue') +  xlab("Survived") + ylim(0, 300)
h1 <- ggplot(na.omit(all), aes(Age)) + geom_histogram(data=subset(na.omit(all), Survived==1), bins=30, aes(fill="blue"), alpha=0.2) + geom_histogram(data=subset(na.omit(all), Survived==0), bins=30, aes(fill="red"), alpha=0.2) + scale_fill_manual(name="group", values=c("blue", "red"), labels=c("Survived", "Died"))
h2 <- ggplot(na.omit(all), aes(Fare)) + geom_histogram(data=subset(na.omit(all), Survived==1), bins=30, aes(fill="blue"), alpha=0.2) + geom_histogram(data=subset(na.omit(all), Survived==0), bins=30, aes(fill="red"), alpha=0.2) + scale_fill_manual(name="group", values=c("blue", "red"), labels=c("Survived", "Died")) + xlim(0, 300)
grid.arrange(
  widths = c(0.75, 1),
  layout_matrix = rbind(c(1, 2), c(3, 4)),
  b1, h1, b2, h2,
  nrow = 2,
  bottom = textGrob(
    "",
    gp = gpar(fontface = 3, fontsize = 9),
    hjust = 1,
    x = 1
  )
)
```

```{r}
lm_age_survived = lm(Age ~ Survived, data=na.omit(all))
anova(lm_age_survived)

lm_fare_survived = lm(Fare ~ Survived, data=na.omit(all))
anova(lm_fare_survived)
```

The outputs of the ANOVA analysis suggest that there are **statistically significant differences in the means of `age` and `fare` of those who survived and did not**. This implies that these two variables could be strong predictors.

#### Dependent Variable (`Survived`) vs. Categorical Independent Variables (`Pclass`, `Sex`, `SibSp`, `Parch`, `Embarked`)

We first convert the following three variables into factor columns: `Pclass`, `SibSp` and `Parch`.

```{r}
all$Pclass <- as.factor(all$Pclass)
all$Ticket <- as.factor(all$Ticket)
all$Sex <- as.factor(all$Sex)
all$SibSp <- as.factor(all$SibSp)
all$Parch <- as.factor(all$Parch)
all$Embarked <- as.factor(all$Embarked)
```

We run pairwise **chi-square tests** for all combinations of the variables (`Survived`) and (`Pclass`, `Sex`, `SibSp`, `Parch`, `Embarked`) to evaluate the strength of correlation.

```{r, message=FALSE, warning=FALSE}
# Run chi-square test for all  of categorical variables
library(plyr)
chi_df <- na.omit(all) %>% dplyr::select(Survived, Pclass, Sex, SibSp, Parch, Embarked)
chi_df <- droplevels(chi_df)
combos <- combn(ncol(chi_df), 2)
corelations <- adply(combos, 2, function(x) {
  column_one <- names(chi_df)[x[1]]
  column_two <- names(chi_df)[x[2]]
  mydata <- data.frame(chi_df[, x[1]], chi_df[, x[2]])
  mytab <- table(mydata)
  chi_test <- chisq.test(mytab)
  out <- data.frame('Column.A' = column_one,
                    'Column.B' = column_two,
                    'p.value' = chi_test$p.value)
  return(out)
})

# Correlation between (`Survived`) and (`Pclass`, `Sex`, `SibSp`, `Parch`, `Embarked`)
corelations %>% filter(Column.A == 'Survived') %>% arrange(p.value)
```

**Chi-square tests** of independence show strong dependencies between `Survived` and variables (`Sex`, `Pclass`, `SibSp`, `Embarked`, `Parch`).

**Mosaic Plots**

A mosaic plot is a graphical display of the cell frequencies of a contingency table in which the area of boxes of the plot are proportional to the cell frequencies of the contingency table. The colors represent the level of the residual for that cell / combination of levels. More specifically, **blue means there are more observations in that cell than would be expected under the null model (independent). Red means there are fewer observations than would have been expected.**

```{r, warning=FALSE}
# (Survived, Sex)
mosaicplot(~ na.omit(all)$Survived+train$Sex, data=train, shade=TRUE, legend=TRUE, xlab='Survived', ylab='Sex', main='')

# (Survived, Pclass)
mosaicplot(~ na.omit(all)$Survived+train$Pclass, data=train, shade=TRUE, legend=TRUE, xlab='Survived', ylab='Pclass', main='')

# (Survived, SibSp)
mosaicplot(~ na.omit(all)$Survived+train$SibSp, data=train, shade=TRUE, legend=TRUE, xlab='Survived', ylab='SibSp', main='')

# (Survived, Parch)
mosaicplot(~ na.omit(all)$Survived+train$Parch, data=train, shade=TRUE, legend=TRUE, xlab='Survived', ylab='Parch', main='')

# (Survived, Embarked)
mosaicplot(~ na.omit(all)$Survived+train$Embarked, data=train, shade=TRUE, legend=TRUE, xlab='Survived', ylab='Embarked', main='')
```

Female passenger survival rate is a lot higher than the null hypothesis, where we assume there is difference in the survival rates between female and male passengers. 1st class passengers' survival rate is significantly higher than that of the 3rd class passengers. Passengers having only one sibling, one spouse, one child or one parent had a higher survival rate than passengers having multiple family members. Passengers who embarked from Cherbourg have a higher survival rate than those from Southampton. 

### Independent Variables vs Independent Variables

Studying the correlations between two independent variables tells us about the **multi-collinearity (information redundancy)** of those variables. 

#### Independent Numerical Variables vs Independent Numerical Variables: (`Age`, `Fare`)

We look into the correlations between the two numerical variables `Fare` and `Age`. For this, we perform a linear regression and evaluate the goodness of fit (R squared). If R squared is large, then there is a linear relation between the two variable and vice versa if it's small.

```{r, fig.width=4.5, fig.height=3.5, message=FALSE, warning=FALSE}
ggplot(na.omit(all), aes(x=Fare, y=Age)) + geom_point(alpha=0.75) + geom_smooth(method=lm, fill="blue", color="blue")
lm_age_fare = lm(Age ~ Fare, data=na.omit(all))
summary(lm_age_fare)$r.squared
```

The R squared 0.01190687 suggests a weak to non-existing correlation between the two variables.

#### Independent Categorical Variables vs Independent Categorical Variables: (`Sex`, `Parch`, `SibSp`, `Embarked`, `Pclass`)

```{r, message=FALSE, warning=FALSE}
# Run chi-square test for all  of categorical variables
library(plyr)
chi_df <- na.omit(all) %>% dplyr::select(Sex, Pclass, SibSp, Parch, Embarked)
chi_df <- droplevels(chi_df)
combos <- combn(ncol(chi_df), 2)
corelations <- adply(combos, 2, function(x) {
  column_one <- names(chi_df)[x[1]]
  column_two <- names(chi_df)[x[2]]
  mydata <- data.frame(chi_df[, x[1]], chi_df[, x[2]])
  mytab <- table(mydata)
  chi_test <- chisq.test(mytab)
  out <- data.frame('Column.A' = column_one,
                    'Column.B' = column_two,
                    'p.value' = chi_test$p.value)
  return(out)
})

# Correlation between `Sex`, `Pclass`, `SibSp`, `Parch`, `Embarked`
corelations %>% arrange(p.value)
```

**Chi-square tests** of independence show strong dependencies between (`Sex`,`Parch`), (`Sex`,`SibSp`), (`Sex`,`Embarked`), (`Sex`, `Pclass`), (`Pclass`,`Embarked`), (`Pclass`,`SibSp`) and (`SibSp`,`Parch`).

#### Independent Numerical Variables (`Age`, `Fare`) vs Independent Categorical Variables (`Pclass`, `Sex`, `SibSp`, `Parch`, `Embarked`)

**`Age`**

```{r, fig.width=7.5, fig.height=10}
b1 <- ggplot(aes(x=as.factor(Pclass), y=Age), data=na.omit(all)) + geom_boxplot(alpha=0.25, fill='blue', color='blue') + xlab("Pclass")
b2 <- ggplot(aes(x=Sex, y=Age), data=na.omit(all)) + geom_boxplot(alpha=0.25, fill='blue', color='blue') + xlab("Sex")
b3 <- ggplot(aes(x=as.factor(SibSp), y=Age), data=na.omit(all)) + geom_boxplot(alpha=0.25, fill='blue', color='blue') + xlab("SibSp")
b4 <- ggplot(aes(x=as.factor(Parch), y=Age), data=na.omit(all)) + geom_boxplot(alpha=0.25, fill='blue', color='blue') + xlab("Parch")
b5 <- ggplot(aes(x=as.factor(Embarked), y=Age), data=na.omit(all)) + geom_boxplot(data=subset(na.omit(all), Embarked!=''), alpha=0.25, fill='blue', color='blue') + xlab("Embarked")
grid.arrange(
  b1, b2, b3, b4, b5,
  nrow = 3,
  bottom = textGrob(
    "",
    gp = gpar(fontface = 3, fontsize = 9),
    hjust = 1,
    x = 1
  )
)
```

The median `Age` of the passengers increases as the level of class increases. The median of the male passenger age is higher than the female's. 

**`Fare`**

```{r, fig.width=7.5, fig.height=10}
# Exclude Fare > 500 
b1 <- ggplot(aes(x=as.factor(Pclass), y=Fare), data=na.omit(all)) + geom_boxplot(data=subset(na.omit(all), Fare<500), alpha=0.25, fill='blue', color='blue') +  xlab("Pclass")
b2 <- ggplot(aes(x=Sex, y=Fare), data=na.omit(all)) + geom_boxplot(data=subset(na.omit(all), Fare<500), alpha=0.25, fill='blue', color='blue') +  xlab("Sex")
b3 <- ggplot(aes(x=as.factor(SibSp), y=Fare), data=na.omit(all)) + geom_boxplot(data=subset(na.omit(all), Fare<500), alpha=0.25, fill='blue', color='blue') +  xlab("SibSp")
b4 <- ggplot(aes(x=as.factor(Parch), y=Fare), data=na.omit(all)) + geom_boxplot(data=subset(na.omit(all), Fare<500), alpha=0.25, fill='blue', color='blue') +  xlab("Parch")
b5 <- ggplot(aes(x=as.factor(Embarked), y=Fare), data=na.omit(all)) + geom_boxplot(data=subset(na.omit(all), Embarked!='' & Fare<500), alpha=0.25, fill='blue', color='blue') +  xlab("Embarked")
grid.arrange(
  b1, b2, b3, b4, b5,
  nrow = 3,
  bottom = textGrob(
    "",
    gp = gpar(fontface = 3, fontsize = 9),
    hjust = 1,
    x = 1
  )
)
```

First class passengers paid the highest fare. Female passengers paid higher price at median.

# Preprocessing / Feature Engineering

## Transformation

```{r}
str(all)
```

**`Name`**

We split `Name` into `FirstName`, `LastName` and `Title`: e.g. "Braund, Mr. Owen Harris" -> "Owen Harris", "Braund", "Mr.".

```{r}
library(stringr)
all$LastName <- str_split_fixed(all$Name, ",", 2)[,1]
tmp <- str_split_fixed(all$Name, ",", 2)[,2]
all$Title <- trimws(str_split_fixed(tmp, "\\.", 2)[,1])
all$FirstName <- str_split_fixed(tmp, "\\.", 2)[,2]
```

We assigned value `Special` to all `Title` values that are not in (`Mr`, `Miss`, `Mrs`). We will later drop the columns `Name`, `FirstName` and `LastName`.

```{r}
all %>% group_by(Title) %>% dplyr::summarise(n=n()) %>% arrange(desc(n))
all$Title[!(all$Title %in% c('Mr', 'Miss', 'Mrs'))] <- 'Special'
all$Title <- as.factor(all$Title)
```

**`Ticket`**

We extract only alphabet characters from this column: e.g. "PC 17599" -> "PC"

```{r}
all$TicketLetters <- sub("^([[:alpha:]]*).*", "\\1", all$Ticket)
all$TicketLetters[all$TicketLetters == ''] <- 'None'
all$TicketLetters[(all$TicketLetters %in% c('LINE', 'PP', 'AQ', 'P', 'WE', 'Fa', 'LP', 'SCO', 'SO', 'SW'))] <- 'Others'
all %>% dplyr::group_by(TicketLetters) %>% dplyr::summarise(n=n()) %>% arrange(desc(n))
all$TicketLetters <- as.factor(all$TicketLetters)
```

**`Cabin`**

We create a new attribute `CabinClass` with the first alphabet character stored in this column: "C85" -> "C".

```{r}
all$CabinClass <- sub("^([[:alpha:]]*).*", "\\1", all$Cabin)
all$CabinClass[all$CabinClass == ''] <- 'None'
all$CabinClass[(all$CabinClass %in% c('T','G'))] <- 'None'
all %>% dplyr::group_by(CabinClass) %>% dplyr::summarise(n=n()) %>% arrange(desc(n))
all$CabinClass <- as.factor(all$CabinClass)
all <- all %>% dplyr::select(-Cabin)
```

**`FamSizeBinned`**

We will bin the familiy size into three classes: i.e. 'Solo', 'Small' and 'Large'. 

```{r}
all <- all %>% mutate(FamSize = as.numeric(as.character(SibSp)) + as.numeric(as.character(Parch)) + 1)
famsize_df <- all %>% filter(!is.na(Survived))
ggplot(famsize_df, aes(x=FamSize, fill=as.factor(Survived))) + geom_bar(stat='count', position='dodge', alpha=0.5) + scale_x_continuous(breaks=c(1:11)) + labs(x='Family Size') + theme_grey() + scale_fill_manual(name="group", values=c("red", "blue"),labels=c("Died", "Survived"))
```

```{r}
all$FamSizeBinned[all$FamSize == 1] <- 'Solo'
all$FamSizeBinned[1 < all$FamSize & all$FamSize <= 4] <- 'Small'
all$FamSizeBinned[4 < all$FamSize] <- 'Large'
all$FamSizeBinned <- as.factor(all$FamSizeBinned)
```

**Drop some variables**

```{r}
all <- all %>% dplyr::select(-Name, -FirstName, -LastName, -FamSize, -SibSp, -Parch, -Ticket)
str(all)
```

## Normalization

We normalize (scale and center) the two numerical variables: `Age` and `Fare`.

```{r}
library(caret)
preProc <- preProcess(all[,-(1:2)], method=c("center", "scale"))
preProc
all <- predict(preProc, all)
```

## Encoding

We take one-hot encoding of the factor variables.

```{r}
str(all)

# Drop levels with no entries
all <- droplevels(all)

# One-hot encoding using model.matrix()
#all <- as.data.frame(model.matrix( ~ . -1, all))

# Standarize weird column names
#colnames(all) <- make.names(colnames(all))
```

## Cleaning

We drop factor levels with near zero variance.

```{r}
# Remove variables with zero (near zero) variance
drop_zerovar <- nearZeroVar(all)
colnames(all[, c(drop_zerovar)])
if (length(colnames(all[, c(drop_zerovar)]))!=0) {
  all <- all[, -c(drop_zerovar)]
}
```

Drop variables that are perfectly collinear with other variables.

```{r}
# Remove perfectly collinear columns
drop_collinear <- c(rownames(alias(lm(PassengerId ~ . , data=all))$Complete))
drop_collinear
all <- all %>% dplyr::select(-all_of(drop_collinear))
```

## Feature Selection by Importance

We use random forest with 100 trees to quickly evaluate the important features.

```{r, fig.width=5, fig.height=5}
# Run a quick random forest with 100 trees to find important features
library(randomForest)
rf_df <- all %>% filter(!is.na(Survived))
set.seed(1)
model_rf <- randomForest(as.factor(Survived) ~ . -PassengerId, data=rf_df, ntree=100, importance=TRUE)
imp_RF <- importance(model_rf)
imp_DF <- data.frame(Variables=row.names(imp_RF), MDA=imp_RF[,1])
imp_DF <- imp_DF[order(imp_DF$MDA, decreasing=TRUE),]
imp_var <- c(imp_DF[,'Variables'])
ggplot(imp_DF, aes(x=reorder(Variables, MDA), y=MDA, fill=MDA)) + geom_bar(stat='identity') + labs(x='', y='% decrease in Accuracy if variable values are randomely shuffled (permuted)') + coord_flip() + theme(legend.position="none")
```

```{r}
# We drop features with negative importance
important_features <- imp_DF %>% filter(MDA>0) %>% rownames()
important_features
all <- all %>% dplyr::select(PassengerId, Survived, all_of(important_features))
```

# Modeling

We re-split the data set `all` to `train` and `test`.

```{r}
train <- all %>% filter(!is.na(Survived))
test <- all %>% filter(is.na(Survived))
```

## Linear Discrimnant Analysis

LDA uses continuous independent variables and a categorical dependent variable. It tries to find a linear hyperplane that maximizes the ratio between the within class variance and the inter class variance. Following are the fundamental assumptions of LDA (from wiki):

  1. **multivariate normality**: independent variables are distributed normally for each level of the grouping variable.
  2. **homogeneity of variance / covariance (homoscedasticity)**: variances among group variables are the same across levels of predictors. This can be tested with **Box's M** statistic. **Linear discriminant analysis** are typically used when the covariances are equal and **quadratic discriminant analysis** when covariances are not equal.
  3. **multicollinearity**: predictive power can decrease with an increased correlation between predictor variables.
  4. **independence**: observations are assumed to be randomly sampled, and a participant's score on one variable is assumed to be independent of scores on that variable for all other participants.

```{r}
# Linear Discriminant Analysis
library(pROC)
model.lda <- lda(as.factor(Survived) ~ . -PassengerId, data=train)
pred.lda <- predict(model.lda, train)
confusionMatrix(pred.lda$class, as.factor(train$Survived))
plot(model.lda)
roc.lda <- roc(response=train$Survived, predictor=pred.lda$posterior[,1])
par(pty="s")
plot(roc.lda)
auc(roc.lda)
```

```{r}
test$Survived <- predict(model.lda, test)$class
submission.lda <- test %>% dplyr::select(PassengerId, Survived)
write.csv(submission.lda,"submission_lda.csv", row.names=FALSE) # 0.8859
```

## Logistic Regressions

We try two logistic regression models with `caret` and choose the optimal hyperparameter for each models using cross validation. 

We start with a **simple logistic regression**.

```{r}
model.log <- glm(as.factor(Survived) ~ . -PassengerId, data=train, family=binomial)
prob.log <- predict(model.log, train, type="response")
class.log <- rep("0", dim(train)[1])
class.log[prob.log > .5] = "1"
confusionMatrix(as.factor(class.log), as.factor(train$Survived))
roc.log <- roc(response=train$Survived, predictor=prob.log)
par(pty="s")
plot(roc.log)
auc(roc.log)
```

Next, we use **elastic net** logistic regression with penalty both on L1 and L2 norm.

```{r}
## Set training control for model building
set.seed(1)
ctrl <- trainControl(method="repeatedcv", number=5, repeats=10, summaryFunction=twoClassSummary, classProbs=TRUE, savePredictions=TRUE, search="random")
model.elastic <- train(make.names(as.factor(Survived)) ~ . -PassengerId, data=train, method="glmnet", family="binomial", metric="ROC", trControl=ctrl, Length=10)
model.elastic$bestTune
```

```{r}
class.elastic <- predict(model.elastic, train, type='raw')
confusionMatrix(as.factor(class.elastic), as.factor(make.names(train$Survived)))
prob.elastic <- predict(model.elastic, train, type='prob')[,1]
roc.elastic <- roc(response=train$Survived, predictor=prob.elastic)
par(pty="s")
plot(roc.elastic)
auc(roc.elastic)
```

We make a prediction using the simple logistic regression model as it produces a higher `auc`. 

```{r}
test$Survived <- predict(model.log, test)
test$Survived[test$Survived >  0.5] <- 1
test$Survived[test$Survived <= 0.5] <- 0
submission.log <- test %>% dplyr::select(PassengerId, Survived)
write.csv(submission.log, "submission_log.csv", row.names=FALSE)
```

## KNN (Nonparametric Model)

```{r}
library(class)
trControl <- trainControl(method="cv", number=10)
model.knn <- train(as.factor(Survived) ~ . -PassengerId, data=train, method="knn", tuneGrid=expand.grid(k=1:10), trControl=trControl, metric="Accuracy")
model.knn$bestTune
class.knn <- predict(model.knn, train, type='raw')
confusionMatrix(as.factor(class.knn), as.factor(train$Survived))
prob.knn <- predict(model.knn, train, type='prob')[,1]
roc.knn <- roc(response=train$Survived, predictor=prob.knn)
par(pty="s")
plot(roc.knn)
auc(roc.knn)
```

```{r}
test$Survived <- predict(model.knn, test, type='raw')
submission.knn <- test %>% dplyr::select(PassengerId, Survived)
write.csv(submission.knn,"submission_knn.csv", row.names=FALSE)
```

## Support Vector Machine

We use `caret` to find the optimal parameters (**gamma** and **cost**) for our support vector machine. **Gamma** is the free parameter of the Gaussian radial basis function. A smaller **gamma** means a Gaussian with a larger variance. **Cost** is the parameter for the soft margin cost function and it controls the cost of mis-classification. A smaller **cost** makes the cost of mis-classificaiton low: softer margin.

```{r, fig.width=7, fig.height=3.5, message=FALSE, warning=TRUE}
library(e1071)
set.seed(1)
model.svm <- tune(svm, as.factor(Survived) ~ . -PassengerId, data=train, ranges=list(gamma=2^(-5:0), cost=seq(1.0,1.5,0.1)), kernel="radial", probability=TRUE)
print(model.svm)
plot(model.svm)
```

```{r}
class.svm <- predict(model.svm$best.model, train)
pred.svm <- predict(model.svm$best.model, train, probability=TRUE)
prob.svm <- attr(pred.svm, "probabilities")[,1]
confusionMatrix(as.factor(pred.svm), as.factor(train$Survived))
roc.svm <- roc(response=train$Survived, predictor=prob.svm)
par(pty="s")
plot(roc.svm)
auc(roc.svm)
```

```{r}
test$Survived <- predict(model.svm$best.model, test)
submission.svm <- test %>% dplyr::select(PassengerId, Survived)
write.csv(submission.svm,"submission_svm.csv", row.names=FALSE) # 0.75119
```

## Decision Tree

The criteria for a making (or not) a new split in a decision tree is to compare the decrease in the error of the tree with the new split against the **complexity parameter (cp)** times the number of leaves it would yield. If the former is greater, then the split is made.


```{r}
library(rpart)
library(rpart.plot)
set.seed(1)

model.tree <- train(as.factor(Survived) ~ . -PassengerId, data=train, method="rpart", trControl=trainControl("cv", number=10), tuneLength=20)

# Plot model error vs different values of complexity parameter
plot(model.tree)
# Print the best tuning parameter cp that minimize the model Accuracy
model.tree$bestTune
# Plot the final tree model
rpart.plot(model.tree$finalModel)
# Decision rules in the model
model.tree$finalModel

prob.tree <- predict(model.tree, newdata=train, type='prob')[,1]
class.tree <- rep("1", dim(train)[1])
class.tree[prob.tree > .5] = "0"

confusionMatrix(as.factor(class.tree), as.factor(train$Survived))
roc.tree <- roc(response=train$Survived, predictor=prob.tree)
par(pty="s")
plot(roc.tree)
auc(roc.tree)
```

```{r}
test$Survived_ <- predict(model.tree, newdata=test, type='prob')[,1]
test$Survived[test$Survived_ >  0.5] <- 0
test$Survived[test$Survived_ <= 0.5] <- 1
submission.tree <- test %>% dplyr::select(PassengerId, Survived)
write.csv(submission.tree,"submission_tree.csv", row.names=FALSE) # 0.73684
```

## Random Forest

Here, we use 5-fold cross validation. **mtry** is the number of variables randomly sampled as candidates at each split.

```{r}
library(randomForest)
set.seed(5)
control <- trainControl(method="cv", number=5)
model.rf <- train(as.factor(Survived) ~ . -PassengerId, data=train, method="rf", trControl=control, tuneGrid=expand.grid(mtry=2:5))
model.rf$bestTune
prob.rf <- predict(model.rf, train, type='prob')[,1]
class.rf <- rep("1", dim(train)[1])
class.rf[prob.rf > .5] = "0"
confusionMatrix(as.factor(class.rf), as.factor(train$Survived))
roc.rf <- roc(response=train$Survived, predictor=prob.rf)
par(pty="s")
plot(roc.rf)
auc(roc.rf)
```

```{r}
test$Survived <- predict(model.rf, test)
submission.rf <- test %>% dplyr::select(PassengerId, Survived)
write.csv(submission.rf,"submission_rf.csv", row.names=FALSE) # 0.77272
```

We extract feature importance.

```{r}
# Extracting variable importance and make graph with ggplot (looks nicer that the standard varImpPlot)
rf_imp <- varImp(model.rf, scale = FALSE)
rf_imp <- rf_imp$importance
rf_gini <- data.frame(Variables=row.names(rf_imp), MeanDecreaseGini=rf_imp$Overall)
ggplot(top_n(rf_gini, 20, MeanDecreaseGini), aes(x=reorder(Variables, MeanDecreaseGini), y=MeanDecreaseGini, fill=MeanDecreaseGini)) +
        geom_bar(stat='identity') + coord_flip() + theme(legend.position="none") + labs(x="") +
        ggtitle('Variable Importance Random Forest') + theme(plot.title = element_text(hjust = 0.5))
```

## XGBoost

```{r}
library(gbm)
set.seed(1)
control <- trainControl(method="cv", number=5)
model.gbm <- train(as.factor(Survived) ~ . -PassengerId, data=train, method='gbm', trControl=control, verbose=FALSE)
prob.gbm <- predict(model.gbm, train, type='prob')[,1]
class.gbm <- rep("1", dim(train)[1])
class.gbm[prob.gbm > .5] = "0"
confusionMatrix(as.factor(class.gbm), as.factor(train$Survived))
roc.gbm <- roc(response=train$Survived, predictor=prob.gbm)
par(pty="s")
plot(roc.gbm)
auc(roc.gbm)
```

```{r}
test$Survived_ <- predict(model.gbm, test, type='prob')[,1]
test$Survived[test$Survived_ >  0.5] <- 0
test$Survived[test$Survived_ <= 0.5] <- 1
submission.gbm <- test %>% dplyr::select(PassengerId, Survived)
write.csv(submission.gbm,"submission_gbm.csv", row.names=FALSE) # 
```

## Neural Network

We use `caret`'s method `nnet` to run a 5-fold cross validation to find the best number of nodes and weight decay factor.

```{r}
library(neuralnet)
#http://sebastianderi.com/aexam/hld_MODEL_neural.html

set.seed(1)

# Step 1: SELECT TUNING PARAMETERS
# Set range of tuning parameters (layer size [number of nodes] and weight decay)
tune_grid_neural <- expand.grid(size=c(1:5), decay=c(0, 0.05, 0.1, 0.5, 1))

# Set other constrains to be imposed on network (to keep computation manageable)
max_size_neaural <- max(tune_grid_neural$size)
max_weights_neural <- max_size_neaural*(nrow(train) + 1) + max_size_neaural + 1

# Step 2: SELECT TUNING METHOD
# set up train control object, which specifies training/testing technique
control_neural <- trainControl(method="cv", number=5)

# Step 3: TRAIN MODEL
model.nn <- train(as.factor(Survived) ~ . -PassengerId, data=train, method="nnet", tuneGrid=tune_grid_neural, trControl=control_neural, trace=FALSE)
```

```{r}
prob.nn <- predict(model.nn, train, type='prob')[,1]
class.nn <- rep("1", dim(train)[1])
class.nn[prob.nn > .5] = "0"
confusionMatrix(as.factor(class.nn), as.factor(train$Survived))
roc.nn <- roc(response=train$Survived, predictor=prob.nn)
par(pty="s")
plot(roc.nn)
auc(roc.nn)
```

```{r}
test$Survived_ <- predict(model.nn, test, type='prob')[,1]
test$Survived[test$Survived_ >  0.5] <- 0
test$Survived[test$Survived_ <= 0.5] <- 1
submission.nn <- test %>% dplyr::select(PassengerId, Survived)
write.csv(submission.nn,"submission_nn.csv", row.names=FALSE) # 0.77272
```

## Combine Models

### Correlation Between the Model Predictions

```{r}
#compose correlations plot
library(corrplot)

results <- as.data.frame(submission.lda[,1:2])
names(results)[1] <- 'PassengerId'
names(results)[2] <- 'lda'
results$log <- submission.log[,2]
results$knn <- submission.knn[,2]
results$svm <- submission.svm[,2]
results$tree <- submission.tree[,2]
results$rf <- submission.rf[,2]
results$gbm <- submission.gbm[,2]
results$nn <- submission.nn[,2]

results$lda <- as.numeric(as.character(results$lda)) # 0.8859
results$log <- as.numeric(as.character(results$log)) # 0.8876
results$knn <- as.numeric(as.character(results$knn)) # 0.938
results$svm <- as.numeric(as.character(results$svm)) # 0.8871
results$tree <- as.numeric(results$tree)             # 0.8956
results$rf <- as.numeric(as.character(results$rf))   # 0.9465
results$gbm <- as.numeric(as.character(results$gbm)) # 0.9216
results$nn <- as.numeric(as.character(results$nn))   # 0.8725

corrplot.mixed(cor(results[, c('lda', 'log', 'knn', 'svm', 'tree', 'rf', 'gbm', 'nn')]), order="hclust", tl.col="black")
```

### Ensembling 1: Majority Voting Between Multiple Models

We select 3 models (svm, rf, gbm) that perform well and have relatively weaker correlation with each other. We take the majority vote between the three models and use the result as our final prediction. 

```{r}
results <- results %>% mutate_if(is.numeric, ~replace(., is.na(.), 0))
results <- transform(results, majority=ifelse(svm + rf + gbm > 1, 1, 0))
submission.majority <- results %>% dplyr::select(PassengerId, majority)
names(submission.majority)[2] <- 'Survived'
write.csv(submission.majority,"submission_majority.csv", row.names=FALSE)
```

### Ensembling 2: Weighted Majority Voting Between Multiple Models

There is another way to take an ensemble of predictions from different models. Here, we will take the svm predictions unless both rf and gbm disagree with the svm prediction.

```{r}
submission.disagree.svm <- results %>% dplyr::select(PassengerId, svm, rf, gbm)
submission.disagree.svm$Survived <- ifelse(results$svm==results$gbm | results$svm==results$rf, results$svm, results$rf)
submission.disagree.svm <- submission.disagree.svm %>% dplyr::select(PassengerId, Survived)
write.csv(submission.disagree.svm,"submission_disagree_svm.csv", row.names=FALSE)
```

Checkout this excellent blog post about practical ensembling techniques: https://mlwave.com/kaggle-ensembling-guide/ 
